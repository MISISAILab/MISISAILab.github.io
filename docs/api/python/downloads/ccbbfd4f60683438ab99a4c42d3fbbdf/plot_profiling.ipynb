{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n\n# Profile the execution of a simple model\n\n*ONNX Runtime* can profile the execution of the model.\nThis example shows how to interpret the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import numpy\nimport onnx\n\nimport onnxruntime as rt\nfrom onnxruntime.datasets import get_example\n\n\ndef change_ir_version(filename, ir_version=6):\n    \"onnxruntime==1.2.0 does not support opset <= 7 and ir_version > 6\"\n    with open(filename, \"rb\") as f:\n        model = onnx.load(f)\n    model.ir_version = 6\n    if model.opset_import[0].version <= 7:\n        model.opset_import[0].version = 11\n    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's load a very simple model and compute some prediction.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "example1 = get_example(\"mul_1.onnx\")\nonnx_model = change_ir_version(example1)\nonnx_model_str = onnx_model.SerializeToString()\nsess = rt.InferenceSession(onnx_model_str, providers=rt.get_available_providers())\ninput_name = sess.get_inputs()[0].name\n\nx = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=numpy.float32)\nres = sess.run(None, {input_name: x})\nprint(res)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We need to enable to profiling\nbefore running the predictions.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "options = rt.SessionOptions()\noptions.enable_profiling = True\nsess_profile = rt.InferenceSession(onnx_model_str, options, providers=rt.get_available_providers())\ninput_name = sess.get_inputs()[0].name\n\nx = numpy.array([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]], dtype=numpy.float32)\n\nsess.run(None, {input_name: x})\nprof_file = sess_profile.end_profiling()\nprint(prof_file)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The results are stored un a file in JSON format.\nLet's see what it contains.\n\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": false
      },
      "outputs": [],
      "source": [
        "import json\n\nwith open(prof_file, \"r\") as f:\n    sess_time = json.load(f)\nimport pprint\n\npprint.pprint(sess_time)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}