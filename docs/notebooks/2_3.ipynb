{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_3.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jilBh_x1aO31"
      },
      "source": [
        "# Automatic differentiation\n",
        "In this task you'll need to use results from previous exercises - compare the analytical answers to them with those, which obtained with any automatic differentiation framework (autograd\\jax\\pytorch\\tensorflow)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yj2jRY3xdGpS"
      },
      "source": [
        "## (a)\n",
        "Implement analytical expression of the gradient and hessian of \n",
        "\n",
        "$$\n",
        "f(x) = \\dfrac{1}{2}x^TAx + b^Tx + c\n",
        "$$\n",
        "\n",
        "and compare it with autograd version.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "s33ph706f94S",
        "outputId": "80cd5b8a-d347-4cff-f184-20e3092392f5"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 10\n",
        "A = np.random.rand((n,n))\n",
        "b = np.random.rand(n)\n",
        "c = np.random.rand(n)\n",
        "\n",
        "\n",
        "def f(x):\n",
        "    # Your code here\n",
        "    return 0\n",
        "\n",
        "def analytical_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def analytical_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "def autograd_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def autograd_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "x_test = np.random.rand(n)\n",
        "\n",
        "print(f'Analytical and autograd implementations of the gradients are close: {np.allclose(analytical_df(x_test), autograd_df(x_test))}')\n",
        "print(f'Analytical and autograd implementations of the hessians are close: {np.allclose(analytical_ddf(x_test), autograd_ddf(x_test))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical and autograd implementations of the gradients are close: True\n",
            "Analytical and autograd implementations of the hessians are close: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "30SAINmfdZvE"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1N3YgssbVeb"
      },
      "source": [
        "##  (b)\n",
        "Implement analytical expression of the gradient and hessian of \n",
        "\n",
        "$$ \n",
        "f(x) = \\ln \\left( 1 + \\exp\\langle a,x\\rangle\\right) \n",
        "$$\n",
        "\n",
        "and compare it with autograd version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ohJZqAuRh0Ef",
        "outputId": "0e8e0aa1-c792-4e7e-844f-06990966e799"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "n = 10\n",
        "a = np.random.rand(n)\n",
        "\n",
        "def f(x):\n",
        "    # Your code here\n",
        "    return 0\n",
        "\n",
        "def analytical_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def analytical_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "def autograd_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def autograd_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "x_test = np.random.rand(n)\n",
        "\n",
        "print(f'Analytical and autograd implementations of the gradients are close: {np.allclose(analytical_df(x_test), autograd_df(x_test))}')\n",
        "print(f'Analytical and autograd implementations of the hessians are close: {np.allclose(analytical_ddf(x_test), autograd_ddf(x_test))}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Analytical and autograd implementations of the gradients are close: True\n",
            "Analytical and autograd implementations of the hessians are close: True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JoAHq3pDiK23"
      },
      "source": [
        "## (c)\n",
        "\n",
        "Implement analytical expression of the gradient and hessian of \n",
        "\n",
        "$$\n",
        "f(x) = \\dfrac{1}{2} \\|Ax - b\\|^2_2\n",
        "$$\n",
        "\n",
        "and compare it with autograd version."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5HdRQNgph1Pj"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "m = 30\n",
        "n = 20\n",
        "A = np.random.rand((m,n))\n",
        "b = np.random.rand(m)\n",
        "\n",
        "def f(x):\n",
        "    # Your code here\n",
        "    return 0\n",
        "\n",
        "def analytical_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def analytical_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "def autograd_df(x):\n",
        "    # Your code here\n",
        "    return np.zeros(n)\n",
        "\n",
        "def autograd_ddf(x):\n",
        "    # Your code here\n",
        "    return np.zeros((n,n))\n",
        "\n",
        "x_test = np.random.rand(n)\n",
        "\n",
        "print(f'Analytical and autograd implementations of the gradients are close: {np.allclose(analytical_df(x_test), autograd_df(x_test))}')\n",
        "print(f'Analytical and autograd implementations of the hessians are close: {np.allclose(analytical_ddf(x_test), autograd_ddf(x_test))}')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bbwbe-isMchj"
      },
      "source": [
        "# Materials\n",
        "* [Jax autodiff cookbook](https://colab.research.google.com/github/google/jax/blob/main/docs/notebooks/autodiff_cookbook.ipynb)\n",
        "* [A Gentle Introduction to ``torch.autograd``](https://colab.research.google.com/github/pytorch/tutorials/blob/gh-pages/_downloads/009cea8b0f40dfcb55e3280f73b06cc2/autograd_tutorial.ipynb)"
      ]
    }
  ]
}