<!-- ---
title: Тест
nav_order: 2
---

# Вступительное тестирование

## Вопросы - минимум

В этом разделе представлены те вопросы, на которые следует знать ответ для того, чтобы слушать курс.

### **1. Можно ли умножить вектор на вектор?**

- [ ] Да, существует несколько способов это сделать - скалярное, векторное произведение.
- [ ] Нет, вектор надо умножать на матрицу или число.
- [ ] Нет, для векторов всё по-другому работает.
- [ ] Да, умножаем каждую компоненту вектора друг на друга. Это называется скалярное
- [ ] произведение.

### 2. Может ли норма матрицы быть равна нулю?

- [ ] Да
- [ ] Нет

### 3. Чему равна производная функции $f(x) = x^2$?

- [ ] $2x$
- [ ] $2x + const$
- [ ] $\frac{x^3}{3}$
- [ ] $\frac{x^3}{3} + const$

Невозможно взять производную.

### 4. Чему равна первообразная функции $f(x) = x^2$?

- [ ] $2x$
- [ ] $2x + const$
- [ ] $\frac{x^3}{3}$
- [ ] $\frac{x^3}{3} + const$

Невозможно вычислить первообразную.

### 5. Чему равно скалярное произведение векторов $(1, 1, 1)$ и $(2,3,4)$?

- [ ] (1,2,1,3,1,4)
- [ ] (1,2,3,4)
- [ ] 9
- [ ] (2,3,4)
- [ ] Невозможно посчитать

### 6. Как посчитать определитель диагональной матрицы?

- [ ] Сложить все диагональные элементы
- [ ] Умножить все диагональные элементы
- [ ] Он равен нулю
- [ ] Определитель такой матрицы равен самой матрице.

## Вопросы по существу курса.

Если вы уверенно знаете ответы на бОльшую часть предложенных ниже вопросов - вероятно, курс будет слишком легким для вас.

### 7. Является ли функция $f(x) = |x|$ выпуклой?

- [ ] Да
- [ ] Нет

### 8. Является ли множество симметричных положительно определенных квадратных матриц выпуклым?

- [ ] Да
- [ ] Нет

### 9. Чему равен субградиент функции $f(x) = \sin(x-4) + 2|x-4|$ в точке $x = 4$?

- [ ] Функция не дифференцируема в этой точке, значит, субградиента не существует.
- [ ] 4
- [ ] Любое число в интервале [-2, 2]
- [ ] Любое число в интервале [-1,1]
- [ ] 0
- [ ] Среди вариантов ответов нет верного
- [ ] Что такое субградиент? (не знаю)
- [ ] Любое число в интервале [-1, 3]

### 10. Вы обучаете нейросеть классифицировать изображения. Размер обучающей выборки 10000, размер батча 100. Сколько эпох вы сделаете, если произведете 1000 итераций стохастического градиентного спуска?

- [ ] 1
- [ ] 10
- [ ] 100
- [ ] 1000
- [ ] 10000
- [ ] Эпоха? (не знаю)
- [ ] Среди вариантов ответов нет верного

### 11. Логистическая регрессия - это метод решения задачи

- [ ] Классификации
- [ ] Регрессии
- [ ] Кластеризации

### 12. Пусть решение задачи линейного программирования существует. Симплекс метод в худшем случае:

- [ ] Не сойдется
- [ ] Сойдется полиномиально
- [ ] Сойдется экспоненциально

### 13. Является ли задача оптимизации весов нейросети ResNet выпуклой?

- [ ] Да
- [ ] Нет
- [ ] Данных задачи недостаточно

### 14. При оптимизации с помощью стохастического градиентного метода было бы хорошей идеей :

- [ ] Уменьшать learning rate со временем
- [ ] Увеличивать learning rate со временем
- [ ] Не изменять learning rate

### 15. Истинно ли утверждение: "Добавление регуляризации Тихонова к выпуклой функции делает функцию сильно выпуклой"?

- [ ] Да
- [ ] Нет

### 16. Найдите минимальную константу Липшица функции $f(x) = Ax - b$, где $x$ - вектор размерности $n$, $А$ - вещественная матрица размерности$m \times n$, $b$ – вектор размерности $m$.

- [ ] Функция не является Липшициевой.
- [ ] $2 \Vert A \Vert$
- [ ] $\Vert A^\top A\Vert$
- [ ] $\Vert A\Vert $
- [ ] $e^{\Vert A\Vert}$
- [ ] $\Vert Ax - b\Vert$

### 17. Верно ли, что метод Ньютона сойдется для выпуклой функции, если запустить его из любой точки пространства.

- [ ] Да
- [ ] Нет

### 18. Пусть вычисление значения функции потерь вашей нейронной сети (forward pass) занимает время t. Скажите, сколько примерно по времени займет вычисление градиентов по весам (backward pass)

- [ ] $t$
- [ ] $2t$
- [ ] $t^2$
- [ ] $\frac{t}{2}$
- [ ] $e^t$
- [ ] $0$
- [ ] $-t$
- [ ] $N_{weights} \cdot t$

### 19. Верно ли утверждение: Nesterov momentum и Polyak Momentum одинаково ускоряют метод градиентного спуска для выпуклой функции с Липшициевым градиентом с точки зрения характера сходимости (с точностью до константного множителя)

- [ ] Да
- [ ] Нет
 -->
